{"instruction": "What are the target modules in LoRA?", "output": "Target modules specify which layers receive LoRA adapters. For transformers, common targets are q_proj (query), v_proj (value), k_proj (key), and o_proj (output) in attention layers. Targeting all four provides best results but uses more memory.", "metadata": {"source": "curated", "topic": "architecture"}}
{"instruction": "What is the purpose of LoRA alpha?", "output": "LoRA alpha is a scaling factor that controls the magnitude of adapter updates. It is typically set to 2x the rank (e.g., alpha=32 for r=16). The actual scaling applied is alpha/r, which determines how much the adapters influence the base model.", "metadata": {"source": "curated", "topic": "lora_config"}}
{"instruction": "What learning rate should be used for QLoRA?", "output": "QLoRA typically uses learning rates between 1e-4 and 5e-4, which is 10x higher than full fine-tuning. Start with 2e-4 and adjust based on loss curves. Use cosine or linear decay with 3-5 percent warmup steps.", "metadata": {"source": "curated", "topic": "hyperparameters"}}
{"instruction": "Explain gradient checkpointing in QLoRA", "output": "Gradient checkpointing trades computation for memory by recomputing activations during backpropagation instead of storing them. This reduces memory usage by 30-50 percent with minimal speed impact, essential for training large models on consumer GPUs.", "metadata": {"source": "curated", "topic": "memory_optimization"}}
{"instruction": "How does 4-bit quantization work in QLoRA?", "output": "QLoRA uses NF4 (Normal Float 4-bit) quantization to compress the base model weights to 4 bits while keeping LoRA adapters in higher precision (bfloat16). This reduces memory footprint while preserving training quality through proper dequantization during forward passes.", "metadata": {"source": "curated", "topic": "quantization"}}
{"instruction": "How do you prevent overfitting in LoRA training?", "output": "Use LoRA dropout (0.05-0.1), reduce the number of training epochs (3-5), increase the dataset size, use gradient clipping (max_grad_norm=1.0), monitor validation loss, and consider using smaller rank values to reduce adapter capacity.", "metadata": {"source": "curated", "topic": "training_best_practices"}}
{"instruction": "What is QLoRA?", "output": "QLoRA (Quantized Low-Rank Adaptation) is a parameter-efficient fine-tuning method that combines 4-bit quantization with LoRA to reduce memory usage by up to 75% while maintaining model quality.", "metadata": {"source": "curated", "topic": "qlora_basics"}}
{"instruction": "Explain the LoRA rank parameter", "output": "The LoRA rank (r) controls the dimensionality of the low-rank adapter matrices. Common values range from 8 to 64. Higher ranks increase model capacity but also memory usage. Rank 16 with alpha=32 is a typical starting point.", "metadata": {"source": "curated", "topic": "lora_config"}}
