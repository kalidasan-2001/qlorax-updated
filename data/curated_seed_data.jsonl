{"instruction": "What is QLoRA and how does it reduce memory requirements?", "output": "QLoRA (Quantized Low-Rank Adaptation) is a parameter-efficient fine-tuning technique that reduces memory requirements by using 4-bit quantization for the base model while training low-rank adapters. This allows fine-tuning of large language models on consumer GPUs with significantly reduced memory footprint.", "metadata": {"source": "curated", "topic": "qlora_basics"}}
{"instruction": "Explain the difference between LoRA and QLoRA.", "output": "LoRA (Low-Rank Adaptation) trains low-rank matrices as adapters while keeping the base model frozen. QLoRA extends this by quantizing the base model to 4-bit precision, further reducing memory usage. This makes QLoRA more memory-efficient than LoRA, enabling fine-tuning of larger models on limited hardware.", "metadata": {"source": "curated", "topic": "qlora_comparison"}}
{"instruction": "What are the key hyperparameters in QLoRA training?", "output": "Key QLoRA hyperparameters include: lora_r (rank of adaptation matrices, typically 8-64), lora_alpha (scaling factor, often 2x rank), lora_dropout (regularization, 0.05-0.1), and bits (quantization precision, usually 4-bit). Learning rate typically ranges from 1e-5 to 3e-4.", "metadata": {"source": "curated", "topic": "qlora_hyperparameters"}}
